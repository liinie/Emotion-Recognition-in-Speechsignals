{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 480)\n",
      "(480,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('dataset_short_time_energy.npy').item()\n",
    "X = dataset['data'].T # shape = (frames, datasample numbers)\n",
    "print(X.shape)\n",
    "m = X.shape[1] # numbers of datasamples\n",
    "Y = np.array(dataset['target'])\n",
    "print(Y.shape)\n",
    "C = len(set(Y)) # number of classes\n",
    "Y_origin = pd.factorize(Y)[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_converter(Y, C):\n",
    "    d = np.eye(C)[Y.reshape(-1)].T\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 480)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = one_hot_converter(Y_origin, C)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomize X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# permutation = list(np.random.permutation(m))\n",
    "# shuffled_X = X[:, permutation]\n",
    "# shuffled_Y = Y[:, permutation].reshape((C, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate the data to training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = shuffled_X[:, :-20]\n",
    "# X_test = shuffled_X[:, -20:]\n",
    "# Y_train = shuffled_Y[:, :-20]\n",
    "# Y_test = shuffled_Y[:, -20:]\n",
    "# assert X_train.shape[1] == Y_train.shape[1], \"train set shape error\"\n",
    "# assert X_test.shape[1] == Y_test.shape[1], \"test set shape error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X[:, :-20]\n",
    "# X_test = X[:, -20:]\n",
    "# Y_train = Y[:, :-20]\n",
    "# Y_test = Y[:, -20:]\n",
    "# assert X_train.shape[1] == Y_train.shape[1], \"train set shape error\"\n",
    "# assert X_test.shape[1] == Y_test.shape[1], \"test set shape error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# Y = np.array([[0, 1, 0], [1, 0, 1]])       \n",
    "# layers_dims = [3, 4, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_choices = {'hidden_size':[40, 60, 80, 100], 'learning_rate':[7.5e-3, 1e-3, 5e-2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_parameter_selection():\n",
    "    return {k:random.choice(v) for k, v in parameter_choices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = random_parameter_selection()\n",
    "type(params['hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 80, 7]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = [X.shape[0], params['hidden_size'], 7] # in this experiment, we always keep one hidden layer\n",
    "layers_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(10**(-2))\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):        \n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(np.sqrt(2./layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = initialization(layers_dims)\n",
    "print(f'shape of W1: {parameters[\"W1\"].shape}')\n",
    "print(f'shape of W2: {parameters[\"W2\"].shape}')\n",
    "print(f'shape of b1: {parameters[\"b1\"].shape}')\n",
    "print(f'shape of b2: {parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    Z = np.dot(W, X) + b\n",
    "    cache = (X, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z1, cache = linear_forward(X, parameters[\"W1\"], parameters[\"b1\"])\n",
    "dataframe0 = pd.DataFrame(Z1)\n",
    "# print(dataframe0)\n",
    "# print(np.isnan(Z1).any()) # check if there is any nan number in Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (np.exp(-1 * Z))\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "#     Z = (Z - np.average(Z))/np.max(Z) \n",
    "    Z = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    A = np.exp(Z)/np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(X, W, b, activation):\n",
    "    Z, cache = linear_forward(X, W, b)\n",
    "#     print(f\"the new Z is {Z- np.max(Z)}\")\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    if activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A1, cache1 = linear_activation_forward(X, parameters[\"W1\"], parameters[\"b1\"], activation='relu')\n",
    "# dataframe1 = pd.DataFrame(A1)\n",
    "# print(dataframe1)\n",
    "# print(np.isnan(A1).any())\n",
    "# print(f\"A1 is {A1}\")\n",
    "# print(f\"X is {cache1[0][0]}\")\n",
    "# print(f\"W1 is {cache1[0][1]}\")\n",
    "# print(f\"b1 is {cache1[0][2]}\")\n",
    "# print(f\"Z1 is {cache1[1]}\")\n",
    "A2, cache2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], activation='softmax')\n",
    "# dataframe2 = pd.DataFrame(A2)\n",
    "# print(dataframe2)\n",
    "# print(np.isnan(A2).any())\n",
    "# print(np.where(np.isnan(A2)))\n",
    "# print(f\"A2 is{A2}\")\n",
    "# print(f\"A1 is {cache2[0][0]}\")\n",
    "# print(f\"W2 is {cache2[0][1]}\")\n",
    "# print(f\"b2 is {cache2[0][2]}\")\n",
    "# print(f\"Z2 is {cache2[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" +str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" +str(L)], activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(X, parameters)\n",
    "dataframeL = pd.DataFrame(AL)\n",
    "print(dataframeL)\n",
    "print(np.isnan(AL).any())\n",
    "print(f'A2 is {AL}')\n",
    "print(f'X is {caches[0][0][0]}')\n",
    "print(f'W1 is {caches[0][0][1]}')\n",
    "print(f\"b1 is {caches[0][0][2]}\")\n",
    "print(f\"Z1 is {caches[0][1]}\")\n",
    "\n",
    "print(f\"A1 is {caches[1][0][0]}\")\n",
    "print(f\"W2 is {caches[1][0][1]}\")\n",
    "print(f\"b2 is {caches[1][0][2]}\")\n",
    "print(f\"Z2 is {caches[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(AL, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backwards(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "  \n",
    "     \n",
    "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "    db = 1/m * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "   \n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dZ2 = A2 - Y\n",
    "grads = {}\n",
    "dA2, dW2, db2 = linear_backwards(dZ2, cache2[0])\n",
    "# print(f'dZ2 is {dZ2}')\n",
    "# print(f'dA2 is {dA2}')\n",
    "# print(f'dW2 is {dW2}')\n",
    "# print(f'db2 is {db2}')\n",
    "\n",
    "grads['dA2'] = dA2\n",
    "grads['dW2'] = dW2\n",
    "grads['db2'] = db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    " \n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ1 = relu_backward(dA2, cache1[1])\n",
    "dZ1 \n",
    "\n",
    "dA1, dW1, db1 = linear_backwards(dZ1, cache1[0])\n",
    "\n",
    "# print(f'dZ1 is {dZ1}')\n",
    "# print(f'dA1 is {dA1}')\n",
    "# print(f'dW1 is {dW1}')\n",
    "# print(f'db1 is {db1}')\n",
    "\n",
    "grads['dA1'] = dA1\n",
    "grads['dW1'] = dW1\n",
    "grads['db1'] = db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "       \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "     \n",
    "    \n",
    "    # Shorten the code\n",
    "    dA_prev, dW, db = linear_backwards(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, cache):\n",
    "    grads = {}\n",
    "    L = len(cache)\n",
    "#     print(L)\n",
    "    m = Y.shape[1]\n",
    "    current_cache = cache[-1]\n",
    "    dZL = AL - Y #gradient back to the layer before softmax\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backwards(dZL, current_cache[0]) #gradient of the last linear layer\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = cache[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" +str(L)], current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = back_prop(AL, Y, caches)\n",
    "grads['db1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "#     print(parameters['b1'].shape)\n",
    "#     print(grads['db1'].shape)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = update_parameters(initialization(layers_dims) , grads, 0.0075)\n",
    "parameters[\"b1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    costs = []\n",
    "    grads = {}\n",
    "    parameters = initialization(layers_dims)\n",
    "#     print(parameters)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"softmax\")\n",
    "        \n",
    "#         print(cache1)\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dZL = A2 - Y\n",
    "        dA2, dW2, db2 = linear_backwards(dZL, cache2[0])\n",
    "        dA1, dW1, db1 = linear_activation_backward(dA2, cache1, 'relu')\n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#         print(parameters)\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialization(layers_dims)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "for i in range(20):\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "    print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): #lr was 0.009\n",
    "    np.random.seed(1)\n",
    "    costs = []                      \n",
    "    \n",
    "#     parameters = initialization(layers_dims)\n",
    "    parameters = he_initialization(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = forward_prop(X, parameters)     \n",
    "        cost = compute_cost(AL, Y)       \n",
    "            \n",
    "        grads = back_prop(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    if print_cost:\n",
    "        plt.plot(np.squeeze(costs), \"o\")\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-d226c30d0cc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    " parameters = L_layer_model(X_train, Y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    AL, caches = forward_prop(X, parameters)\n",
    "    prediction = np.argmax(AL, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-08c4fe599947>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions_X_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_X_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_X_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"length of {}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions_X_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_X_train = predict(parameters, X_train)\n",
    "print(predictions_X_train)\n",
    "print(np.argmax(Y_train, axis=0))\n",
    "assert len(predictions_X_train) == Y_train.shape[1], \"length of {}\"\n",
    "predictions_X_test = predict(parameters, X_test)\n",
    "print(predictions_X_test)\n",
    "print(np.argmax(Y_test, axis=0))\n",
    "assert len(predictions_X_test) == Y_test.shape[1], \"length of {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, Y):\n",
    "    count = 0\n",
    "    for i in zip(predictions, Y):\n",
    "        if i[0] == i[1]:\n",
    "            count += 1\n",
    "    accuracy = count /len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = accuracy(predictions_X_train, np.argmax(Y_train, axis=0))\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = accuracy(predictions_X_test, np.argmax(Y_test, axis=0))\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(exp_num):\n",
    "    for i in range(exp_num):\n",
    "        params = random_parameter_selection()\n",
    "        layers_dims = [X.shape[0], params['hidden_size'], 7]\n",
    "        parameters = L_layer_model(X_train, Y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=False)\n",
    "        accuracy_test = accuracy(predict(parameters, X_test), np.argmax(Y_test, axis=0))\n",
    "\n",
    "        experiment_dir = os.path.abspath(\"./experiments/{}_accuracy_{}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'),accuracy_test))\n",
    "        if not os.path.exists(experiment_dir):\n",
    "            os.makedirs(experiment_dir)\n",
    "\n",
    "        parameter_data_file = os.path.join(experiment_dir, \"parameter.json\")\n",
    "\n",
    "        with open(parameter_data_file, 'w') as f:\n",
    "            json.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 2\n",
    "save_results_to_file(exp_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 476) (480, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = X.T\n",
    "y = Y.T\n",
    "print(X.shape, y.shape)\n",
    "kf = KFold(n_splits=9, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file_with_ht_and_cv(exp_num):\n",
    "    best_f1_score = 0\n",
    "    for i in tnrange(exp_num, desc='1st loop'):\n",
    "        params = random_parameter_selection()\n",
    "        layers_dims = [476, params['hidden_size'], 7]\n",
    "        \n",
    "#         accuracy_train = []\n",
    "#         accuracy_val = []\n",
    "        f1_score_val = []\n",
    "        \n",
    "        # cross validataion with KFold n_splits\n",
    "        for train_index, val_index in kf.split(X_trainval):\n",
    "            X_train, X_val = X[train_index].T, X[val_index].T\n",
    "            y_train, y_val = y[train_index].T, y[val_index].T\n",
    "#             print(f'X_train: {X_train.shape},\\nX_test: {X_test.shape},\\ny_train: {y_train.shape},\\ny_test:{y_test.shape}')\n",
    "            \n",
    "            parameters = L_layer_model(X_train, y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=False)\n",
    "            prediction_X_train = predict(parameters, X_train)\n",
    "            prediction_X_val = predict(parameters, X_val)\n",
    "#             accuracy_train.append(accuracy(prediction_X_train, np.argmax(y_train, axis=0)))\n",
    "#             accuracy_val.append(accuracy(prediction_X_val, np.argmax(y_val, axis=0)))\n",
    "            f1_score_val.append(f1_score(np.argmax(y_val, axis=0), prediction_X_val, average='micro'))\n",
    "    \n",
    "#         accuracy_train_mean = np.array(accuracy_train).mean()\n",
    "#         accuracy_val_mean = np.array(accuracy_val).mean()  \n",
    "        f1_score_val_mean = np.array(f1_score_val).mean()\n",
    "        \n",
    "        if f1_score_val_mean > best_f1_score:\n",
    "            best_f1_score = f1_score_val_mean\n",
    "            best_params = params\n",
    "        \n",
    "#         experiment_dir = os.path.abspath(\"./experiments/{}_accuracy_{}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'),accuracy_val_mean))\n",
    "        experiment_dir = os.path.abspath(\"./experiments/{}_f1_score_{:.3f}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'), f1_score_val_mean))\n",
    "\n",
    "        if not os.path.exists(experiment_dir):\n",
    "                os.makedirs(experiment_dir)\n",
    "\n",
    "        parameter_data_file = os.path.join(experiment_dir, \"parameter.json\")\n",
    "\n",
    "        with open(parameter_data_file, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    return best_f1_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ada36ceaa14fb1a4f5ad8b6773c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "exp_num = 50\n",
    "best_f1_score, best_params = save_results_to_file_with_ht_and_cv(exp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 28.153707\n",
      "Cost after iteration 100: 1.148794\n",
      "Cost after iteration 200: 1.019735\n",
      "Cost after iteration 300: 0.937108\n",
      "Cost after iteration 400: 0.862373\n",
      "Cost after iteration 500: 0.803661\n",
      "Cost after iteration 600: 0.770679\n",
      "Cost after iteration 700: 0.723783\n",
      "Cost after iteration 800: 0.699065\n",
      "Cost after iteration 900: 0.675898\n",
      "Cost after iteration 1000: 0.658769\n",
      "Cost after iteration 1100: 0.639122\n",
      "Cost after iteration 1200: 0.623110\n",
      "Cost after iteration 1300: 0.604667\n",
      "Cost after iteration 1400: 0.592553\n",
      "Cost after iteration 1500: 0.575945\n",
      "Cost after iteration 1600: 0.563464\n",
      "Cost after iteration 1700: 0.549203\n",
      "Cost after iteration 1800: 0.540747\n",
      "Cost after iteration 1900: 0.534534\n",
      "Cost after iteration 2000: 0.523951\n",
      "Cost after iteration 2100: 0.518198\n",
      "Cost after iteration 2200: 0.512161\n",
      "Cost after iteration 2300: 0.506276\n",
      "Cost after iteration 2400: 0.496368\n",
      "Cost after iteration 2500: 0.492518\n",
      "Cost after iteration 2600: 0.489064\n",
      "Cost after iteration 2700: 0.485922\n",
      "Cost after iteration 2800: 0.483183\n",
      "Cost after iteration 2900: 0.480111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdfb1338d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_weights_bias = L_layer_model(X_trainval.T, y_trainval.T, [476, best_params['hidden_size'], 7], learning_rate=best_params['learning_rate'], num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 476)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_X_test = predict(best_weights_bias, X_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 48)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.312\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 48]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-750fb0f7c141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Micro average f1 score: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Macro average f1 score: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 48]"
     ]
    }
   ],
   "source": [
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(np.argmax(y_test, axis=0), predict_X_test, average='micro')))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, predict_X_test, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         3\n",
      "          1       0.00      0.00      0.00         6\n",
      "          2       0.00      0.00      0.00         8\n",
      "          3       0.50      0.20      0.29         5\n",
      "          4       0.39      0.80      0.52        15\n",
      "          5       0.20      0.17      0.18         6\n",
      "          6       0.50      0.20      0.29         5\n",
      "\n",
      "avg / total       0.25      0.31      0.25        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=0), predict_X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
