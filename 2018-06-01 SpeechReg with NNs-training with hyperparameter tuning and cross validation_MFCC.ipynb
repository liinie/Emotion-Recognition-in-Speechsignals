{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8556, 480)\n",
      "(480,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('dataset_MFCC.npy').item()\n",
    "X = dataset['data'].T # shape = (frames, datasample numbers)\n",
    "print(X.shape)\n",
    "m = X.shape[1] # numbers of datasamples\n",
    "Y = np.array(dataset['target'])\n",
    "print(Y.shape)\n",
    "C = len(set(Y)) # number of classes\n",
    "Y_origin = pd.factorize(Y)[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_converter(Y, C):\n",
    "    d = np.eye(C)[Y.reshape(-1)].T\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 480)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = one_hot_converter(Y_origin, C)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomize X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "permutation = list(np.random.permutation(m))\n",
    "shuffled_X = X[:, permutation]\n",
    "shuffled_Y = Y[:, permutation].reshape((C, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate the data to training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = shuffled_X[:, :-20]\n",
    "X_test = shuffled_X[:, -20:]\n",
    "Y_train = shuffled_Y[:, :-20]\n",
    "Y_test = shuffled_Y[:, -20:]\n",
    "assert X_train.shape[1] == Y_train.shape[1], \"train set shape error\"\n",
    "assert X_test.shape[1] == Y_test.shape[1], \"test set shape error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X[:, :-20]\n",
    "# X_test = X[:, -20:]\n",
    "# Y_train = Y[:, :-20]\n",
    "# Y_test = Y[:, -20:]\n",
    "# assert X_train.shape[1] == Y_train.shape[1], \"train set shape error\"\n",
    "# assert X_test.shape[1] == Y_test.shape[1], \"test set shape error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# Y = np.array([[0, 1, 0], [1, 0, 1]])       \n",
    "# layers_dims = [3, 4, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_choices = {'hidden_size':[40, 60, 80, 100], 'learning_rate':[7.5e-3, 1e-3, 5e-2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_parameter_selection():\n",
    "    return {k:random.choice(v) for k, v in parameter_choices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = random_parameter_selection()\n",
    "type(params['hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8556, 40, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = [X.shape[0], params['hidden_size'], 7] # in this experiment, we always keep one hidden layer\n",
    "layers_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(10**(-2))\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):        \n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(np.sqrt(2./layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters = initialization(layers_dims)\n",
    "# print(f'shape of W1: {parameters[\"W1\"].shape}')\n",
    "# print(f'shape of W2: {parameters[\"W2\"].shape}')\n",
    "# print(f'shape of b1: {parameters[\"b1\"].shape}')\n",
    "# print(f'shape of b2: {parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    Z = np.dot(W, X) + b\n",
    "    cache = (X, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Z1, cache = linear_forward(X, parameters[\"W1\"], parameters[\"b1\"])\n",
    "# dataframe0 = pd.DataFrame(Z1)\n",
    "# print(dataframe0)\n",
    "# print(np.isnan(Z1).any()) # check if there is any nan number in Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (np.exp(-1 * Z))\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "#     Z = (Z - np.average(Z))/np.max(Z) \n",
    "    Z = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    A = np.exp(Z)/np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(X, W, b, activation):\n",
    "    Z, cache = linear_forward(X, W, b)\n",
    "#     print(f\"the new Z is {Z- np.max(Z)}\")\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    if activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "        cache = (cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A1, cache1 = linear_activation_forward(X, parameters[\"W1\"], parameters[\"b1\"], activation='relu')\n",
    "# dataframe1 = pd.DataFrame(A1)\n",
    "# print(dataframe1)\n",
    "# print(np.isnan(A1).any())\n",
    "# print(f\"A1 is {A1}\")\n",
    "# print(f\"X is {cache1[0][0]}\")\n",
    "# print(f\"W1 is {cache1[0][1]}\")\n",
    "# print(f\"b1 is {cache1[0][2]}\")\n",
    "# print(f\"Z1 is {cache1[1]}\")\n",
    "# A2, cache2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], activation='softmax')\n",
    "# dataframe2 = pd.DataFrame(A2)\n",
    "# print(dataframe2)\n",
    "# print(np.isnan(A2).any())\n",
    "# print(np.where(np.isnan(A2)))\n",
    "# print(f\"A2 is{A2}\")\n",
    "# print(f\"A1 is {cache2[0][0]}\")\n",
    "# print(f\"W2 is {cache2[0][1]}\")\n",
    "# print(f\"b2 is {cache2[0][2]}\")\n",
    "# print(f\"Z2 is {cache2[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" +str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" +str(L)], activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL, caches = forward_prop(X, parameters)\n",
    "# dataframeL = pd.DataFrame(AL)\n",
    "# print(dataframeL)\n",
    "# print(np.isnan(AL).any())\n",
    "# print(f'A2 is {AL}')\n",
    "# print(f'X is {caches[0][0][0]}')\n",
    "# print(f'W1 is {caches[0][0][1]}')\n",
    "# print(f\"b1 is {caches[0][0][2]}\")\n",
    "# print(f\"Z1 is {caches[0][1]}\")\n",
    "\n",
    "# print(f\"A1 is {caches[1][0][0]}\")\n",
    "# print(f\"W2 is {caches[1][0][1]}\")\n",
    "# print(f\"b2 is {caches[1][0][2]}\")\n",
    "# print(f\"Z2 is {caches[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost(AL, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backwards(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "  \n",
    "     \n",
    "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "    db = 1/m * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "   \n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dZ2 = A2 - Y\n",
    "# grads = {}\n",
    "# dA2, dW2, db2 = linear_backwards(dZ2, cache2[0])\n",
    "# # print(f'dZ2 is {dZ2}')\n",
    "# # print(f'dA2 is {dA2}')\n",
    "# # print(f'dW2 is {dW2}')\n",
    "# # print(f'db2 is {db2}')\n",
    "\n",
    "# grads['dA2'] = dA2\n",
    "# grads['dW2'] = dW2\n",
    "# grads['db2'] = db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    " \n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dZ1 = relu_backward(dA2, cache1[1])\n",
    "# dZ1 \n",
    "\n",
    "# dA1, dW1, db1 = linear_backwards(dZ1, cache1[0])\n",
    "\n",
    "# # print(f'dZ1 is {dZ1}')\n",
    "# # print(f'dA1 is {dA1}')\n",
    "# # print(f'dW1 is {dW1}')\n",
    "# # print(f'db1 is {db1}')\n",
    "\n",
    "# grads['dA1'] = dA1\n",
    "# grads['dW1'] = dW1\n",
    "# grads['db1'] = db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "       \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "     \n",
    "    \n",
    "    # Shorten the code\n",
    "    dA_prev, dW, db = linear_backwards(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, cache):\n",
    "    grads = {}\n",
    "    L = len(cache)\n",
    "#     print(L)\n",
    "    m = Y.shape[1]\n",
    "    current_cache = cache[-1]\n",
    "    dZL = AL - Y #gradient back to the layer before softmax\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backwards(dZL, current_cache[0]) #gradient of the last linear layer\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = cache[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" +str(L)], current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = back_prop(AL, Y, caches)\n",
    "# grads['db1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "#     print(parameters['b1'].shape)\n",
    "#     print(grads['db1'].shape)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = update_parameters(initialization(layers_dims) , grads, 0.0075)\n",
    "# parameters[\"b1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "#     costs = []\n",
    "#     grads = {}\n",
    "#     parameters = initialization(layers_dims)\n",
    "# #     print(parameters)\n",
    "#     W1 = parameters[\"W1\"]\n",
    "#     b1 = parameters[\"b1\"]\n",
    "#     W2 = parameters[\"W2\"]\n",
    "#     b2 = parameters[\"b2\"]\n",
    "    \n",
    "#     for i in range(num_iterations):\n",
    "        \n",
    "#         A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "#         A2, cache2 = linear_activation_forward(A1, W2, b2, \"softmax\")\n",
    "        \n",
    "# #         print(cache1)\n",
    "        \n",
    "#         cost = compute_cost(A2, Y)\n",
    "        \n",
    "#         dZL = A2 - Y\n",
    "#         dA2, dW2, db2 = linear_backwards(dZL, cache2[0])\n",
    "#         dA1, dW1, db1 = linear_activation_backward(dA2, cache1, 'relu')\n",
    "        \n",
    "#         grads['dW1'] = dW1\n",
    "#         grads['db1'] = db1\n",
    "#         grads['dW2'] = dW2\n",
    "#         grads['db2'] = db2\n",
    "        \n",
    "#         parameters = update_parameters(parameters, grads, learning_rate)\n",
    "# #         print(parameters)\n",
    "#         W1 = parameters[\"W1\"]\n",
    "#         b1 = parameters[\"b1\"]\n",
    "#         W2 = parameters[\"W2\"]\n",
    "#         b2 = parameters[\"b2\"]\n",
    "        \n",
    "#         if print_cost and i % 100 == 0:\n",
    "#             print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "#         if print_cost and i % 100 == 0:\n",
    "#             costs.append(cost)\n",
    "       \n",
    "#     # plot the cost\n",
    "\n",
    "#     plt.plot(np.squeeze(costs))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = initialization(layers_dims)\n",
    "# W1 = parameters[\"W1\"]\n",
    "# b1 = parameters[\"b1\"]\n",
    "# W2 = parameters[\"W2\"]\n",
    "# b2 = parameters[\"b2\"]\n",
    "# for i in range(20):\n",
    "#     A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "#     print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): #lr was 0.009\n",
    "    np.random.seed(1)\n",
    "    costs = []                      \n",
    "    \n",
    "#     parameters = initialization(layers_dims)\n",
    "    parameters = he_initialization(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = forward_prop(X, parameters)     \n",
    "        cost = compute_cost(AL, Y)       \n",
    "            \n",
    "        grads = back_prop(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    if print_cost:\n",
    "        plt.plot(np.squeeze(costs), \"o\")\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.753644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 1.916760\n",
      "Cost after iteration 200: 1.911248\n",
      "Cost after iteration 300: 1.910514\n",
      "Cost after iteration 400: 1.910424\n",
      "Cost after iteration 500: 1.910413\n",
      "Cost after iteration 600: 1.910412\n",
      "Cost after iteration 700: 1.910412\n",
      "Cost after iteration 800: 1.910412\n",
      "Cost after iteration 900: 1.910412\n",
      "Cost after iteration 1000: 1.910412\n",
      "Cost after iteration 1100: 1.910412\n",
      "Cost after iteration 1200: 1.910412\n",
      "Cost after iteration 1300: 1.910412\n",
      "Cost after iteration 1400: 1.910412\n",
      "Cost after iteration 1500: 1.910412\n",
      "Cost after iteration 1600: 1.910412\n",
      "Cost after iteration 1700: 1.910412\n",
      "Cost after iteration 1800: 1.910412\n",
      "Cost after iteration 1900: 1.910412\n",
      "Cost after iteration 2000: 1.910412\n",
      "Cost after iteration 2100: 1.910412\n",
      "Cost after iteration 2200: 1.910412\n",
      "Cost after iteration 2300: 1.910412\n",
      "Cost after iteration 2400: 1.910412\n",
      "Cost after iteration 2500: 1.910412\n",
      "Cost after iteration 2600: 1.910412\n",
      "Cost after iteration 2700: 1.910412\n",
      "Cost after iteration 2800: 1.910412\n",
      "Cost after iteration 2900: 1.910412\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c64d4ae390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " parameters = L_layer_model(X_train, Y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    AL, caches = forward_prop(X, parameters)\n",
    "    prediction = np.argmax(AL, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 0 1 1 3 1 2 4 6 1 5 6 4 4 0 6 4 5 4 6 6 5 0 2 2 4 5 6 4 6 4 0 3 6 1 3 4\n",
      " 0 4 0 0 4 4 4 0 0 4 0 1 4 4 4 4 0 6 5 6 6 6 4 5 6 4 2 4 5 4 4 3 5 1 3 6 6\n",
      " 0 3 6 3 2 4 4 4 0 0 5 3 6 2 0 0 0 3 3 6 5 4 0 1 6 4 5 2 3 3 1 4 1 6 4 0 3\n",
      " 2 3 3 3 5 4 2 5 5 4 4 2 2 2 3 4 3 4 1 2 0 2 0 6 1 3 0 1 4 1 3 0 4 6 4 2 4\n",
      " 4 6 5 2 5 4 5 6 6 3 2 3 4 4 3 6 4 4 5 3 4 0 4 0 5 4 4 2 5 1 1 2 2 3 6 6 4\n",
      " 2 4 4 6 1 5 1 0 2 1 1 4 0 2 6 5 5 0 4 3 5 6 3 2 0 0 6 0 2 3 1 4 5 1 0 3 6\n",
      " 2 0 5 0 2 5 2 2 2 5 4 4 3 4 3 4 5 6 3 4 6 0 5 3 3 1 4 4 1 5 2 1 1 4 6 5 4\n",
      " 5 1 6 2 2 2 1 1 5 5 5 3 5 5 0 4 5 4 6 1 4 1 5 5 0 6 3 2 1 2 2 6 4 3 5 0 3\n",
      " 2 6 6 2 4 6 5 6 1 2 1 4 6 1 1 6 1 6 1 0 0 1 3 1 6 4 0 4 0 2 3 4 4 0 5 2 3\n",
      " 4 4 4 4 4 1 0 0 4 1 2 5 3 2 2 3 4 1 1 2 0 5 4 0 0 4 1 2 0 4 3 5 6 4 6 3 1\n",
      " 5 4 5 4 3 4 4 3 5 0 4 2 6 1 1 5 4 6 0 4 6 1 4 4 4 0 3 4 4 1 5 4 1 6 6 3 3\n",
      " 4 1 1 4 2 6 1 4 2 4 2 1 3 4 0 3 1 4 0 2 2 0 4 2 2 0 4 5 3 4 2 4 1 3 4 3 5\n",
      " 5 4 5 4 0 6 2 1 4 5 1 3 5 5 3 6]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 4 4 2 4 4 4 3 0 2 5 0 6 4 0 4 4 6 3 3]\n"
     ]
    }
   ],
   "source": [
    "predictions_X_train = predict(parameters, X_train)\n",
    "print(predictions_X_train)\n",
    "print(np.argmax(Y_train, axis=0))\n",
    "assert len(predictions_X_train) == Y_train.shape[1], \"length of {}\"\n",
    "predictions_X_test = predict(parameters, X_test)\n",
    "print(predictions_X_test)\n",
    "print(np.argmax(Y_test, axis=0))\n",
    "assert len(predictions_X_test) == Y_test.shape[1], \"length of {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(predictions, Y):\n",
    "#     count = 0\n",
    "#     for i in zip(predictions, Y):\n",
    "#         if i[0] == i[1]:\n",
    "#             count += 1\n",
    "#     accuracy = count /len(predictions)\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train = accuracy(predictions_X_train, np.argmax(Y_train, axis=0))\n",
    "# accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_test = accuracy(predictions_X_test, np.argmax(Y_test, axis=0))\n",
    "# accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_results_to_file(exp_num):\n",
    "#     for i in range(exp_num):\n",
    "#         params = random_parameter_selection()\n",
    "#         layers_dims = [X.shape[0], params['hidden_size'], 7]\n",
    "#         parameters = L_layer_model(X_train, Y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=False)\n",
    "#         accuracy_test = accuracy(predict(parameters, X_test), np.argmax(Y_test, axis=0))\n",
    "\n",
    "#         experiment_dir = os.path.abspath(\"./experiments/{}_accuracy_{}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'),accuracy_test))\n",
    "#         if not os.path.exists(experiment_dir):\n",
    "#             os.makedirs(experiment_dir)\n",
    "\n",
    "#         parameter_data_file = os.path.join(experiment_dir, \"parameter.json\")\n",
    "\n",
    "#         with open(parameter_data_file, 'w') as f:\n",
    "#             json.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_num = 2\n",
    "# save_results_to_file(exp_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 8556) (480, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = X.T\n",
    "y = Y.T\n",
    "print(X.shape, y.shape)\n",
    "kf = KFold(n_splits=9, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file_with_ht_and_cv(exp_num):\n",
    "    best_f1_score = 0\n",
    "    for i in tnrange(exp_num, desc='1st loop'):\n",
    "        params = random_parameter_selection()\n",
    "        layers_dims = [X.shape[1], params['hidden_size'], 7]\n",
    "        \n",
    "#         accuracy_train = []\n",
    "#         accuracy_val = []\n",
    "        f1_score_val = []\n",
    "        \n",
    "        # cross validataion with KFold n_splits\n",
    "        for train_index, val_index in kf.split(X_trainval):\n",
    "            X_train, X_val = X[train_index].T, X[val_index].T\n",
    "            y_train, y_val = y[train_index].T, y[val_index].T\n",
    "#             print(f'X_train: {X_train.shape},\\nX_test: {X_test.shape},\\ny_train: {y_train.shape},\\ny_test:{y_test.shape}')\n",
    "            \n",
    "            parameters = L_layer_model(X_train, y_train, layers_dims, learning_rate=params['learning_rate'], num_iterations=3000, print_cost=False)\n",
    "            prediction_X_train = predict(parameters, X_train)\n",
    "            prediction_X_val = predict(parameters, X_val)\n",
    "#             accuracy_train.append(accuracy(prediction_X_train, np.argmax(y_train, axis=0)))\n",
    "#             accuracy_val.append(accuracy(prediction_X_val, np.argmax(y_val, axis=0)))\n",
    "            f1_score_val.append(f1_score(np.argmax(y_val, axis=0), prediction_X_val, average='micro'))\n",
    "    \n",
    "#         accuracy_train_mean = np.array(accuracy_train).mean()\n",
    "#         accuracy_val_mean = np.array(accuracy_val).mean()  \n",
    "        f1_score_val_mean = np.array(f1_score_val).mean()\n",
    "        \n",
    "        if f1_score_val_mean > best_f1_score:\n",
    "            best_f1_score = f1_score_val_mean\n",
    "            best_params = params\n",
    "        \n",
    "#         experiment_dir = os.path.abspath(\"./experiments/{}_accuracy_{}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'),accuracy_val_mean))\n",
    "        experiment_dir = os.path.abspath(\"./experiments/{}_f1_score_{:.3f}\".format(datetime.now().strftime('%Y%m%d_%H%M%S_%f'), f1_score_val_mean))\n",
    "\n",
    "        if not os.path.exists(experiment_dir):\n",
    "                os.makedirs(experiment_dir)\n",
    "\n",
    "        parameter_data_file = os.path.join(experiment_dir, \"parameter.json\")\n",
    "\n",
    "        with open(parameter_data_file, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "    return best_f1_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b76857f63d419fb15939a7635cbb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "exp_num = 50\n",
    "best_f1_score, best_params = save_results_to_file_with_ht_and_cv(exp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 97.432403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 1.916911\n",
      "Cost after iteration 200: 1.911408\n",
      "Cost after iteration 300: 1.910663\n",
      "Cost after iteration 400: 1.910567\n",
      "Cost after iteration 500: 1.910554\n",
      "Cost after iteration 600: 1.910553\n",
      "Cost after iteration 700: 1.910552\n",
      "Cost after iteration 800: 1.910552\n",
      "Cost after iteration 900: 1.910552\n",
      "Cost after iteration 1000: 1.910552\n",
      "Cost after iteration 1100: 1.910552\n",
      "Cost after iteration 1200: 1.910552\n",
      "Cost after iteration 1300: 1.910552\n",
      "Cost after iteration 1400: 1.910552\n",
      "Cost after iteration 1500: 1.910552\n",
      "Cost after iteration 1600: 1.910552\n",
      "Cost after iteration 1700: 1.910552\n",
      "Cost after iteration 1800: 1.910552\n",
      "Cost after iteration 1900: 1.910552\n",
      "Cost after iteration 2000: 1.910552\n",
      "Cost after iteration 2100: 1.910552\n",
      "Cost after iteration 2200: 1.910552\n",
      "Cost after iteration 2300: 1.910552\n",
      "Cost after iteration 2400: 1.910552\n",
      "Cost after iteration 2500: 1.910552\n",
      "Cost after iteration 2600: 1.910552\n",
      "Cost after iteration 2700: 1.910552\n",
      "Cost after iteration 2800: 1.910552\n",
      "Cost after iteration 2900: 1.910552\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c64d5d0d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_weights_bias = L_layer_model(X_trainval.T, y_trainval.T, [8556, best_params['hidden_size'], 7], learning_rate=best_params['learning_rate'], num_iterations=3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_X_test = predict(best_weights_bias, X_test.T)\n",
    "predict_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 48)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.T\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.312\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 48]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-750fb0f7c141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Micro average f1 score: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Macro average f1 score: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 48]"
     ]
    }
   ],
   "source": [
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(np.argmax(y_test, axis=0), predict_X_test, average='micro')))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, predict_X_test, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\speechrecognition\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         3\n",
      "          1       0.00      0.00      0.00         6\n",
      "          2       0.00      0.00      0.00         8\n",
      "          3       0.00      0.00      0.00         5\n",
      "          4       0.31      1.00      0.48        15\n",
      "          5       0.00      0.00      0.00         6\n",
      "          6       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.10      0.31      0.15        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=0), predict_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
